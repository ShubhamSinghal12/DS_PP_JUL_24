{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d1e32b-ae7b-4ee1-a554-cfd93c787b60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92bd9373-e3f8-48c2-90fd-c6c6e71b252c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74c8424e-7fb9-4026-abc3-6f61715dd377",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./data1.csv',header = None)\n",
    "labels = pd.read_csv('./label1.csv',header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d039db70-ce59-4291-9f0f-3b425f7038d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.446891</td>\n",
       "      <td>-0.013397</td>\n",
       "      <td>0.232645</td>\n",
       "      <td>2.156649</td>\n",
       "      <td>1.652923</td>\n",
       "      <td>-0.210531</td>\n",
       "      <td>-0.662227</td>\n",
       "      <td>0.705144</td>\n",
       "      <td>1.434684</td>\n",
       "      <td>-0.445750</td>\n",
       "      <td>-0.178501</td>\n",
       "      <td>-0.905503</td>\n",
       "      <td>-0.594099</td>\n",
       "      <td>-0.728586</td>\n",
       "      <td>-0.627002</td>\n",
       "      <td>-0.240520</td>\n",
       "      <td>0.131696</td>\n",
       "      <td>-0.945878</td>\n",
       "      <td>1.840922</td>\n",
       "      <td>-0.279024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.243747</td>\n",
       "      <td>-1.144175</td>\n",
       "      <td>-0.622214</td>\n",
       "      <td>-0.661979</td>\n",
       "      <td>-0.373315</td>\n",
       "      <td>0.520313</td>\n",
       "      <td>-1.307954</td>\n",
       "      <td>1.639481</td>\n",
       "      <td>-2.838816</td>\n",
       "      <td>0.331636</td>\n",
       "      <td>0.100261</td>\n",
       "      <td>-1.173067</td>\n",
       "      <td>-0.079774</td>\n",
       "      <td>2.194899</td>\n",
       "      <td>0.242839</td>\n",
       "      <td>0.412164</td>\n",
       "      <td>-0.328714</td>\n",
       "      <td>0.902931</td>\n",
       "      <td>-1.418728</td>\n",
       "      <td>-0.290139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.417799</td>\n",
       "      <td>-0.088833</td>\n",
       "      <td>-0.647181</td>\n",
       "      <td>-1.141497</td>\n",
       "      <td>-1.321100</td>\n",
       "      <td>0.686798</td>\n",
       "      <td>-1.045569</td>\n",
       "      <td>-0.376153</td>\n",
       "      <td>-2.272447</td>\n",
       "      <td>-0.669733</td>\n",
       "      <td>-1.070732</td>\n",
       "      <td>1.091993</td>\n",
       "      <td>0.692293</td>\n",
       "      <td>0.937617</td>\n",
       "      <td>0.262142</td>\n",
       "      <td>0.399238</td>\n",
       "      <td>0.614614</td>\n",
       "      <td>1.209465</td>\n",
       "      <td>-1.526386</td>\n",
       "      <td>0.337771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.656784</td>\n",
       "      <td>0.381201</td>\n",
       "      <td>-1.039011</td>\n",
       "      <td>1.285315</td>\n",
       "      <td>0.808324</td>\n",
       "      <td>1.428519</td>\n",
       "      <td>-1.144367</td>\n",
       "      <td>-0.609958</td>\n",
       "      <td>0.983676</td>\n",
       "      <td>0.060917</td>\n",
       "      <td>0.471647</td>\n",
       "      <td>-0.594279</td>\n",
       "      <td>2.104027</td>\n",
       "      <td>-1.113597</td>\n",
       "      <td>-0.188799</td>\n",
       "      <td>1.226253</td>\n",
       "      <td>-1.067763</td>\n",
       "      <td>-0.350275</td>\n",
       "      <td>-0.746426</td>\n",
       "      <td>-1.258696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.098520</td>\n",
       "      <td>1.706769</td>\n",
       "      <td>-1.030370</td>\n",
       "      <td>2.001009</td>\n",
       "      <td>2.751551</td>\n",
       "      <td>1.202229</td>\n",
       "      <td>-0.941066</td>\n",
       "      <td>1.965282</td>\n",
       "      <td>2.979131</td>\n",
       "      <td>1.402483</td>\n",
       "      <td>-1.432987</td>\n",
       "      <td>0.400322</td>\n",
       "      <td>-0.839851</td>\n",
       "      <td>-1.580974</td>\n",
       "      <td>-0.659587</td>\n",
       "      <td>2.292688</td>\n",
       "      <td>0.494188</td>\n",
       "      <td>-1.583187</td>\n",
       "      <td>-2.468254</td>\n",
       "      <td>-0.469648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19995</th>\n",
       "      <td>0.146324</td>\n",
       "      <td>-0.618555</td>\n",
       "      <td>0.547413</td>\n",
       "      <td>0.851301</td>\n",
       "      <td>0.838880</td>\n",
       "      <td>-1.248402</td>\n",
       "      <td>0.698199</td>\n",
       "      <td>-0.545033</td>\n",
       "      <td>0.994348</td>\n",
       "      <td>-0.054620</td>\n",
       "      <td>0.581239</td>\n",
       "      <td>-1.842074</td>\n",
       "      <td>-0.552879</td>\n",
       "      <td>-0.102096</td>\n",
       "      <td>0.307538</td>\n",
       "      <td>-1.267832</td>\n",
       "      <td>-0.165875</td>\n",
       "      <td>-0.717397</td>\n",
       "      <td>-0.808588</td>\n",
       "      <td>0.796346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19996</th>\n",
       "      <td>-2.227923</td>\n",
       "      <td>-0.327196</td>\n",
       "      <td>1.258849</td>\n",
       "      <td>-0.945254</td>\n",
       "      <td>3.925262</td>\n",
       "      <td>0.316686</td>\n",
       "      <td>-1.061159</td>\n",
       "      <td>1.846706</td>\n",
       "      <td>3.692157</td>\n",
       "      <td>0.698035</td>\n",
       "      <td>-0.184239</td>\n",
       "      <td>-0.546036</td>\n",
       "      <td>-0.457919</td>\n",
       "      <td>-1.030581</td>\n",
       "      <td>2.406014</td>\n",
       "      <td>-0.978943</td>\n",
       "      <td>-0.086879</td>\n",
       "      <td>-1.964453</td>\n",
       "      <td>-0.875574</td>\n",
       "      <td>1.535930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19997</th>\n",
       "      <td>-0.800405</td>\n",
       "      <td>1.477997</td>\n",
       "      <td>0.090057</td>\n",
       "      <td>-0.973997</td>\n",
       "      <td>-0.204970</td>\n",
       "      <td>0.224539</td>\n",
       "      <td>0.302265</td>\n",
       "      <td>0.361146</td>\n",
       "      <td>-1.906636</td>\n",
       "      <td>1.381875</td>\n",
       "      <td>0.034366</td>\n",
       "      <td>1.690199</td>\n",
       "      <td>-0.901569</td>\n",
       "      <td>2.183842</td>\n",
       "      <td>0.831008</td>\n",
       "      <td>0.036308</td>\n",
       "      <td>0.516088</td>\n",
       "      <td>0.378670</td>\n",
       "      <td>0.110968</td>\n",
       "      <td>-0.176363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19998</th>\n",
       "      <td>-1.217344</td>\n",
       "      <td>-0.785401</td>\n",
       "      <td>-0.413781</td>\n",
       "      <td>-0.335324</td>\n",
       "      <td>0.181671</td>\n",
       "      <td>-1.685733</td>\n",
       "      <td>-0.437981</td>\n",
       "      <td>-1.176626</td>\n",
       "      <td>-1.434436</td>\n",
       "      <td>-0.880986</td>\n",
       "      <td>2.117703</td>\n",
       "      <td>-2.136842</td>\n",
       "      <td>-1.185261</td>\n",
       "      <td>1.547704</td>\n",
       "      <td>1.174712</td>\n",
       "      <td>-0.429117</td>\n",
       "      <td>0.132068</td>\n",
       "      <td>0.249277</td>\n",
       "      <td>-0.941496</td>\n",
       "      <td>1.661661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19999</th>\n",
       "      <td>1.601005</td>\n",
       "      <td>-0.890398</td>\n",
       "      <td>-1.425049</td>\n",
       "      <td>0.887501</td>\n",
       "      <td>0.167205</td>\n",
       "      <td>-0.395042</td>\n",
       "      <td>-0.512554</td>\n",
       "      <td>0.050777</td>\n",
       "      <td>-0.034945</td>\n",
       "      <td>-0.289219</td>\n",
       "      <td>0.639189</td>\n",
       "      <td>0.981751</td>\n",
       "      <td>-0.110296</td>\n",
       "      <td>-0.187214</td>\n",
       "      <td>-0.226811</td>\n",
       "      <td>-0.547772</td>\n",
       "      <td>-0.006706</td>\n",
       "      <td>-0.023773</td>\n",
       "      <td>1.220809</td>\n",
       "      <td>1.489330</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20000 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1         2         3         4         5         6   \\\n",
       "0     -0.446891 -0.013397  0.232645  2.156649  1.652923 -0.210531 -0.662227   \n",
       "1      0.243747 -1.144175 -0.622214 -0.661979 -0.373315  0.520313 -1.307954   \n",
       "2     -1.417799 -0.088833 -0.647181 -1.141497 -1.321100  0.686798 -1.045569   \n",
       "3      0.656784  0.381201 -1.039011  1.285315  0.808324  1.428519 -1.144367   \n",
       "4      1.098520  1.706769 -1.030370  2.001009  2.751551  1.202229 -0.941066   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "19995  0.146324 -0.618555  0.547413  0.851301  0.838880 -1.248402  0.698199   \n",
       "19996 -2.227923 -0.327196  1.258849 -0.945254  3.925262  0.316686 -1.061159   \n",
       "19997 -0.800405  1.477997  0.090057 -0.973997 -0.204970  0.224539  0.302265   \n",
       "19998 -1.217344 -0.785401 -0.413781 -0.335324  0.181671 -1.685733 -0.437981   \n",
       "19999  1.601005 -0.890398 -1.425049  0.887501  0.167205 -0.395042 -0.512554   \n",
       "\n",
       "             7         8         9         10        11        12        13  \\\n",
       "0      0.705144  1.434684 -0.445750 -0.178501 -0.905503 -0.594099 -0.728586   \n",
       "1      1.639481 -2.838816  0.331636  0.100261 -1.173067 -0.079774  2.194899   \n",
       "2     -0.376153 -2.272447 -0.669733 -1.070732  1.091993  0.692293  0.937617   \n",
       "3     -0.609958  0.983676  0.060917  0.471647 -0.594279  2.104027 -1.113597   \n",
       "4      1.965282  2.979131  1.402483 -1.432987  0.400322 -0.839851 -1.580974   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "19995 -0.545033  0.994348 -0.054620  0.581239 -1.842074 -0.552879 -0.102096   \n",
       "19996  1.846706  3.692157  0.698035 -0.184239 -0.546036 -0.457919 -1.030581   \n",
       "19997  0.361146 -1.906636  1.381875  0.034366  1.690199 -0.901569  2.183842   \n",
       "19998 -1.176626 -1.434436 -0.880986  2.117703 -2.136842 -1.185261  1.547704   \n",
       "19999  0.050777 -0.034945 -0.289219  0.639189  0.981751 -0.110296 -0.187214   \n",
       "\n",
       "             14        15        16        17        18        19  \n",
       "0     -0.627002 -0.240520  0.131696 -0.945878  1.840922 -0.279024  \n",
       "1      0.242839  0.412164 -0.328714  0.902931 -1.418728 -0.290139  \n",
       "2      0.262142  0.399238  0.614614  1.209465 -1.526386  0.337771  \n",
       "3     -0.188799  1.226253 -1.067763 -0.350275 -0.746426 -1.258696  \n",
       "4     -0.659587  2.292688  0.494188 -1.583187 -2.468254 -0.469648  \n",
       "...         ...       ...       ...       ...       ...       ...  \n",
       "19995  0.307538 -1.267832 -0.165875 -0.717397 -0.808588  0.796346  \n",
       "19996  2.406014 -0.978943 -0.086879 -1.964453 -0.875574  1.535930  \n",
       "19997  0.831008  0.036308  0.516088  0.378670  0.110968 -0.176363  \n",
       "19998  1.174712 -0.429117  0.132068  0.249277 -0.941496  1.661661  \n",
       "19999 -0.226811 -0.547772 -0.006706 -0.023773  1.220809  1.489330  \n",
       "\n",
       "[20000 rows x 20 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ebdc5a25-6d65-4f3d-82e6-a77bd954ec47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19995</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19996</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19997</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19998</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19999</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20000 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0\n",
       "0      3\n",
       "1      2\n",
       "2      2\n",
       "3      3\n",
       "4      3\n",
       "...   ..\n",
       "19995  3\n",
       "19996  1\n",
       "19997  2\n",
       "19998  2\n",
       "19999  0\n",
       "\n",
       "[20000 rows x 1 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae0d8760-c499-48c7-88bf-17fb0c83c456",
   "metadata": {},
   "outputs": [],
   "source": [
    "npData = data.values\n",
    "npLabels = labels.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b0c92e6-bc0c-4466-833f-392cb688973a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "6.873800762086958\n"
     ]
    }
   ],
   "source": [
    "print(np.max(npLabels))\n",
    "print(np.max(npData))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65be641c-6d3d-4966-9214-1eeeb11de5cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(npLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f2c41b8-2f84-4220-abf2-518b406d8794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "(20000, 1, 5)\n"
     ]
    }
   ],
   "source": [
    "num = np.max(npLabels)+1\n",
    "oneHot = np.eye(num)[npLabels]\n",
    "print(oneHot.ndim)\n",
    "print(oneHot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a03a064e-c27d-48a7-9897-291a53e4a1fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " ...\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "oneHotRe = np.reshape(oneHot,[20000,5])\n",
    "print(oneHotRe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e10bd7b-0cf4-433b-82b8-da36a6173b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData, testData, trainLabel, testLabel = train_test_split(npData,oneHotRe,train_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dcb56730-8795-4758-9313-3d5caabdb11d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16000, 20) (4000, 20) (16000, 5) (4000, 5)\n"
     ]
    }
   ],
   "source": [
    "print(trainData.shape,testData.shape,trainLabel.shape,testLabel.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "284c17eb-197c-4f76-9580-cab0bb284f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    def __init__(self,InputL,hiddenLayers,OutputL):\n",
    "\n",
    "        self.weights = []\n",
    "        self.bias = []\n",
    "\n",
    "        self.weights.append(np.random.normal(0,1/np.sqrt(InputL),size=(InputL,hiddenLayers[0])))\n",
    "        self.bias.append(np.zeros((hiddenLayers[0],)))\n",
    "\n",
    "        for i in range(1,len(hiddenLayers)):\n",
    "            self.weights.append(np.random.normal(0,1/np.sqrt(hiddenLayers[i-1]),size=(hiddenLayers[i-1],hiddenLayers[i])))\n",
    "            self.bias.append(np.zeros((hiddenLayers[i],)))\n",
    "\n",
    "        self.weights.append(np.random.normal(0,1/np.sqrt(hiddenLayers[-1]),size=(hiddenLayers[-1],OutputL)))\n",
    "        self.bias.append(np.zeros((OutputL,)))\n",
    "\n",
    "    def sigmoidActivation(self,Z):\n",
    "        return 1.0/(1.0+np.exp(-Z))\n",
    "\n",
    "    def softmaxActivation(self,Z):\n",
    "        temp = np.exp(Z)\n",
    "        return temp/np.sum(temp,axis = 1,keepdims=True)\n",
    "\n",
    "    def fit(self,X,y,lr = 0.01,epochs = 100):\n",
    "        self.backward(X,y,lr,epochs)\n",
    "\n",
    "    def predict(self,X):\n",
    "        ypred = self.forward(X)\n",
    "        ypred[ypred >= 0.5] = 1.0\n",
    "        ypred[ypred < 0.5] = 0.0\n",
    "        return ypred\n",
    "\n",
    "    def Acc(self,y,t,size):\n",
    "        maxData = np.argmax(y,axis = 1)\n",
    "        maxLabel = np.argmax(t,axis = 1)\n",
    "        compare = np.equal(maxData,maxLabel)\n",
    "    \n",
    "        count = np.sum(compare)\n",
    "        return (count/size)*100\n",
    "\n",
    "    def costFn(self,y,t):\n",
    "        return -(t*np.log(y))\n",
    "        \n",
    "    def forward(self,X):\n",
    "        \n",
    "        a = X\n",
    "        self.output = [a]\n",
    "        for i in range(len(self.weights)-1):\n",
    "            z = a@self.weights[i] + self.bias[i]\n",
    "            a = self.sigmoidActivation(z)\n",
    "            self.output.append(a)\n",
    "\n",
    "        z = a@self.weights[-1] + self.bias[-1]\n",
    "        a = self.softmaxActivation(z)\n",
    "        self.output.append(a)\n",
    "            \n",
    "        return a\n",
    "\n",
    "    def derA(self,Z):\n",
    "        return Z*(1-Z)\n",
    "\n",
    "    def backward(self,X,y,lr = 0.01,epochs = 100):\n",
    "        losses = []\n",
    "        for e in range(epochs):\n",
    "            ypred = self.forward(X)\n",
    "    \n",
    "            delta = [ypred - y]\n",
    "            gradw = [self.output[-2].T@delta[-1]]\n",
    "\n",
    "            # print(delta)\n",
    "\n",
    "            for i in range(len(self.weights)-1,0,-1):\n",
    "                delta.append((delta[-1]@self.weights[i].T)*self.derA(self.output[i]))\n",
    "                gradw.append(self.output[i-1].T@delta[-1])\n",
    "\n",
    "\n",
    "            wn = len(self.weights)-1\n",
    "            \n",
    "            for i in range(len(gradw)):\n",
    "                self.weights[wn] = self.weights[wn] - lr*gradw[i]\n",
    "                self.bias[wn] = self.bias[wn] - lr*np.sum(delta[i],axis = 0)\n",
    "                wn -= 1\n",
    "\n",
    "            loss = np.sum(self.costFn(ypred,y))/ypred.shape[0]\n",
    "            print(\"Epoch:\",e,\"Train Loss:\",loss,\"Train Accuracy:\",self.Acc(ypred,y,y.shape[0]))\n",
    "            losses.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6b4c0a92-3fe2-4ff0-b04b-b72f972559a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MLP(trainData.shape[1],[40,30,60],trainLabel.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6689062-0098-472f-99b4-98cb1b003dce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Train Loss: 1.680854951133007 Train Accuracy: 19.83125\n",
      "Epoch: 1 Train Loss: 2.4238454836135293 Train Accuracy: 20.056250000000002\n",
      "Epoch: 2 Train Loss: 4.075129260674188 Train Accuracy: 20.181250000000002\n",
      "Epoch: 3 Train Loss: 3.1441630406705285 Train Accuracy: 19.8125\n",
      "Epoch: 4 Train Loss: 2.890313277566383 Train Accuracy: 21.45\n",
      "Epoch: 5 Train Loss: 2.0605557454882018 Train Accuracy: 20.056250000000002\n",
      "Epoch: 6 Train Loss: 1.739625014030944 Train Accuracy: 20.349999999999998\n",
      "Epoch: 7 Train Loss: 1.6271576320159233 Train Accuracy: 26.09375\n",
      "Epoch: 8 Train Loss: 1.6054970693741615 Train Accuracy: 24.39375\n",
      "Epoch: 9 Train Loss: 1.6031143326550559 Train Accuracy: 22.6125\n",
      "Epoch: 10 Train Loss: 1.602365704432426 Train Accuracy: 22.20625\n",
      "Epoch: 11 Train Loss: 1.601644257803829 Train Accuracy: 22.93125\n",
      "Epoch: 12 Train Loss: 1.6008829358982237 Train Accuracy: 24.0375\n",
      "Epoch: 13 Train Loss: 1.6000747562540678 Train Accuracy: 25.112499999999997\n",
      "Epoch: 14 Train Loss: 1.59921380522419 Train Accuracy: 26.26875\n",
      "Epoch: 15 Train Loss: 1.598293505501113 Train Accuracy: 27.250000000000004\n",
      "Epoch: 16 Train Loss: 1.5973064581160916 Train Accuracy: 28.19375\n",
      "Epoch: 17 Train Loss: 1.5962443030529214 Train Accuracy: 29.05625\n",
      "Epoch: 18 Train Loss: 1.5950975555126643 Train Accuracy: 29.975\n",
      "Epoch: 19 Train Loss: 1.5938554118955834 Train Accuracy: 30.943749999999998\n",
      "Epoch: 20 Train Loss: 1.5925055193267068 Train Accuracy: 31.556250000000002\n",
      "Epoch: 21 Train Loss: 1.5910337013226983 Train Accuracy: 32.074999999999996\n",
      "Epoch: 22 Train Loss: 1.5894236307829346 Train Accuracy: 32.75\n",
      "Epoch: 23 Train Loss: 1.5876564399145823 Train Accuracy: 33.38125\n",
      "Epoch: 24 Train Loss: 1.5857102550400863 Train Accuracy: 34.050000000000004\n",
      "Epoch: 25 Train Loss: 1.5835596426464547 Train Accuracy: 34.61875\n",
      "Epoch: 26 Train Loss: 1.5811749518336287 Train Accuracy: 35.3\n",
      "Epoch: 27 Train Loss: 1.5785215380800717 Train Accuracy: 35.975\n",
      "Epoch: 28 Train Loss: 1.575558854977421 Train Accuracy: 36.6375\n",
      "Epoch: 29 Train Loss: 1.572239406008763 Train Accuracy: 37.40625\n",
      "Epoch: 30 Train Loss: 1.5685075603926193 Train Accuracy: 38.4375\n",
      "Epoch: 31 Train Loss: 1.5642982600387938 Train Accuracy: 39.24375\n",
      "Epoch: 32 Train Loss: 1.559535685809227 Train Accuracy: 40.13125\n",
      "Epoch: 33 Train Loss: 1.5541320209073475 Train Accuracy: 40.98125\n",
      "Epoch: 34 Train Loss: 1.547986561341763 Train Accuracy: 41.8875\n",
      "Epoch: 35 Train Loss: 1.5409855941379416 Train Accuracy: 43.13125\n",
      "Epoch: 36 Train Loss: 1.5330037050523933 Train Accuracy: 44.1625\n",
      "Epoch: 37 Train Loss: 1.5239074794938223 Train Accuracy: 45.125\n",
      "Epoch: 38 Train Loss: 1.5135628582820846 Train Accuracy: 45.9375\n",
      "Epoch: 39 Train Loss: 1.5018475309359822 Train Accuracy: 46.5125\n",
      "Epoch: 40 Train Loss: 1.4886693670557236 Train Accuracy: 46.949999999999996\n",
      "Epoch: 41 Train Loss: 1.4739905653084822 Train Accuracy: 46.90625\n",
      "Epoch: 42 Train Loss: 1.457854661767547 Train Accuracy: 47.025\n",
      "Epoch: 43 Train Loss: 1.4404101937722915 Train Accuracy: 47.03125\n",
      "Epoch: 44 Train Loss: 1.4219221979129637 Train Accuracy: 46.9125\n",
      "Epoch: 45 Train Loss: 1.4027630986009407 Train Accuracy: 46.96875\n",
      "Epoch: 46 Train Loss: 1.3833793691469045 Train Accuracy: 47.081250000000004\n",
      "Epoch: 47 Train Loss: 1.3642385983513436 Train Accuracy: 47.2375\n",
      "Epoch: 48 Train Loss: 1.345769627610053 Train Accuracy: 47.49375\n",
      "Epoch: 49 Train Loss: 1.3283118067324953 Train Accuracy: 47.6875\n",
      "Epoch: 50 Train Loss: 1.3120858439103642 Train Accuracy: 47.949999999999996\n",
      "Epoch: 51 Train Loss: 1.2971900574708786 Train Accuracy: 48.29375\n",
      "Epoch: 52 Train Loss: 1.2836170523076305 Train Accuracy: 48.75\n",
      "Epoch: 53 Train Loss: 1.2712812442556993 Train Accuracy: 49.125\n",
      "Epoch: 54 Train Loss: 1.2600480373325385 Train Accuracy: 49.3\n",
      "Epoch: 55 Train Loss: 1.249758715739209 Train Accuracy: 49.7\n",
      "Epoch: 56 Train Loss: 1.2402486832357196 Train Accuracy: 49.99375\n",
      "Epoch: 57 Train Loss: 1.23135915413457 Train Accuracy: 50.331250000000004\n",
      "Epoch: 58 Train Loss: 1.2229435734018748 Train Accuracy: 50.7\n",
      "Epoch: 59 Train Loss: 1.2148703064576478 Train Accuracy: 51.075\n",
      "Epoch: 60 Train Loss: 1.2070229470704898 Train Accuracy: 51.4375\n",
      "Epoch: 61 Train Loss: 1.1992992557595132 Train Accuracy: 51.849999999999994\n",
      "Epoch: 62 Train Loss: 1.1916094185209982 Train Accuracy: 52.131249999999994\n",
      "Epoch: 63 Train Loss: 1.1838740629348725 Train Accuracy: 52.606249999999996\n",
      "Epoch: 64 Train Loss: 1.1760222920311296 Train Accuracy: 53.09374999999999\n",
      "Epoch: 65 Train Loss: 1.1679898832731819 Train Accuracy: 53.4875\n",
      "Epoch: 66 Train Loss: 1.159717734414513 Train Accuracy: 53.91875\n",
      "Epoch: 67 Train Loss: 1.151150605899037 Train Accuracy: 54.40625\n",
      "Epoch: 68 Train Loss: 1.142236200313891 Train Accuracy: 55.012499999999996\n",
      "Epoch: 69 Train Loss: 1.1329246254314906 Train Accuracy: 55.675\n",
      "Epoch: 70 Train Loss: 1.1231683033893083 Train Accuracy: 56.181250000000006\n",
      "Epoch: 71 Train Loss: 1.1129224072511126 Train Accuracy: 56.818749999999994\n",
      "Epoch: 72 Train Loss: 1.102145926036552 Train Accuracy: 57.425000000000004\n",
      "Epoch: 73 Train Loss: 1.0908034581253696 Train Accuracy: 58.074999999999996\n",
      "Epoch: 74 Train Loss: 1.0788678280038912 Train Accuracy: 58.8\n",
      "Epoch: 75 Train Loss: 1.0663235201964416 Train Accuracy: 59.550000000000004\n",
      "Epoch: 76 Train Loss: 1.053170885788042 Train Accuracy: 60.09375\n",
      "Epoch: 77 Train Loss: 1.0394306665027426 Train Accuracy: 60.91875\n",
      "Epoch: 78 Train Loss: 1.0251486285855476 Train Accuracy: 61.824999999999996\n",
      "Epoch: 79 Train Loss: 1.0103986352332395 Train Accuracy: 62.8\n",
      "Epoch: 80 Train Loss: 0.99528546071229 Train Accuracy: 63.518750000000004\n",
      "Epoch: 81 Train Loss: 0.9799424608759457 Train Accuracy: 64.375\n",
      "Epoch: 82 Train Loss: 0.9645441069534213 Train Accuracy: 65.13125\n",
      "Epoch: 83 Train Loss: 0.9493473163737962 Train Accuracy: 65.86874999999999\n",
      "Epoch: 84 Train Loss: 0.9351304711277094 Train Accuracy: 66.3375\n",
      "Epoch: 85 Train Loss: 0.9255713144710468 Train Accuracy: 66.925\n",
      "Epoch: 86 Train Loss: 0.9433819258427619 Train Accuracy: 63.625\n",
      "Epoch: 87 Train Loss: 1.1098873996412189 Train Accuracy: 51.55\n",
      "Epoch: 88 Train Loss: 1.9077386513156664 Train Accuracy: 33.887499999999996\n",
      "Epoch: 89 Train Loss: 2.132804502186315 Train Accuracy: 28.95625\n",
      "Epoch: 90 Train Loss: 2.235380431245719 Train Accuracy: 34.9\n",
      "Epoch: 91 Train Loss: 1.154101113117187 Train Accuracy: 60.6\n",
      "Epoch: 92 Train Loss: 0.9420729666598775 Train Accuracy: 64.14999999999999\n",
      "Epoch: 93 Train Loss: 0.9733657546702507 Train Accuracy: 65.325\n",
      "Epoch: 94 Train Loss: 0.9941718782739863 Train Accuracy: 61.68749999999999\n",
      "Epoch: 95 Train Loss: 1.1081503784324842 Train Accuracy: 56.49999999999999\n",
      "Epoch: 96 Train Loss: 1.0889618341103255 Train Accuracy: 56.9125\n",
      "Epoch: 97 Train Loss: 1.162761919214341 Train Accuracy: 53.46875\n",
      "Epoch: 98 Train Loss: 1.0474652209383197 Train Accuracy: 58.706250000000004\n",
      "Epoch: 99 Train Loss: 1.0302915047811254 Train Accuracy: 60.59375\n",
      "Epoch: 100 Train Loss: 0.933430733725713 Train Accuracy: 64.6125\n",
      "Epoch: 101 Train Loss: 0.9040670331223086 Train Accuracy: 65.61875\n",
      "Epoch: 102 Train Loss: 0.8580752248156387 Train Accuracy: 68.84375\n",
      "Epoch: 103 Train Loss: 0.8400638713340477 Train Accuracy: 67.8125\n",
      "Epoch: 104 Train Loss: 0.8222315238105505 Train Accuracy: 70.75\n",
      "Epoch: 105 Train Loss: 0.812931301318734 Train Accuracy: 68.88125000000001\n",
      "Epoch: 106 Train Loss: 0.8049388045070353 Train Accuracy: 71.6375\n",
      "Epoch: 107 Train Loss: 0.8004785342597631 Train Accuracy: 69.15\n",
      "Epoch: 108 Train Loss: 0.7968492599373671 Train Accuracy: 71.8\n",
      "Epoch: 109 Train Loss: 0.7968886350257856 Train Accuracy: 68.7125\n",
      "Epoch: 110 Train Loss: 0.7970435192309553 Train Accuracy: 71.5875\n",
      "Epoch: 111 Train Loss: 0.8043208879903024 Train Accuracy: 67.3625\n",
      "Epoch: 112 Train Loss: 0.8091495708976939 Train Accuracy: 70.2125\n",
      "Epoch: 113 Train Loss: 0.8295212716768521 Train Accuracy: 65.0625\n",
      "Epoch: 114 Train Loss: 0.8367801683341693 Train Accuracy: 68.01875\n",
      "Epoch: 115 Train Loss: 0.8744947863514091 Train Accuracy: 62.075\n",
      "Epoch: 116 Train Loss: 0.8694734583636655 Train Accuracy: 66.04375\n",
      "Epoch: 117 Train Loss: 0.9129816727589739 Train Accuracy: 60.475\n",
      "Epoch: 118 Train Loss: 0.878146285395766 Train Accuracy: 65.71249999999999\n",
      "Epoch: 119 Train Loss: 0.9051056593734361 Train Accuracy: 60.95625\n",
      "Epoch: 120 Train Loss: 0.8553180728867396 Train Accuracy: 66.93124999999999\n",
      "Epoch: 121 Train Loss: 0.8630520054472752 Train Accuracy: 62.931250000000006\n",
      "Epoch: 122 Train Loss: 0.823036529572471 Train Accuracy: 68.50625000000001\n",
      "Epoch: 123 Train Loss: 0.8219115295776972 Train Accuracy: 65.16875\n",
      "Epoch: 124 Train Loss: 0.7960034819224118 Train Accuracy: 70.06875000000001\n",
      "Epoch: 125 Train Loss: 0.7926497731431565 Train Accuracy: 67.10625\n",
      "Epoch: 126 Train Loss: 0.7763040066436732 Train Accuracy: 71.24374999999999\n",
      "Epoch: 127 Train Loss: 0.7730642500252366 Train Accuracy: 68.56875\n",
      "Epoch: 128 Train Loss: 0.7622452004457203 Train Accuracy: 72.0125\n",
      "Epoch: 129 Train Loss: 0.759763059681116 Train Accuracy: 69.54375\n",
      "Epoch: 130 Train Loss: 0.7520958477975133 Train Accuracy: 72.53125\n",
      "Epoch: 131 Train Loss: 0.7505077533945606 Train Accuracy: 70.1375\n",
      "Epoch: 132 Train Loss: 0.7446284635957089 Train Accuracy: 72.95\n",
      "Epoch: 133 Train Loss: 0.7439195759035683 Train Accuracy: 70.5875\n",
      "Epoch: 134 Train Loss: 0.7389206803775143 Train Accuracy: 73.1625\n",
      "Epoch: 135 Train Loss: 0.7389971263984261 Train Accuracy: 70.90625\n",
      "Epoch: 136 Train Loss: 0.7341321325902777 Train Accuracy: 73.38125\n",
      "Epoch: 137 Train Loss: 0.7348213101353822 Train Accuracy: 71.14375\n",
      "Epoch: 138 Train Loss: 0.7294130493302509 Train Accuracy: 73.49375\n",
      "Epoch: 139 Train Loss: 0.7304712252803829 Train Accuracy: 71.40625\n",
      "Epoch: 140 Train Loss: 0.7239737590583945 Train Accuracy: 73.8375\n",
      "Epoch: 141 Train Loss: 0.7251290661064147 Train Accuracy: 71.72500000000001\n",
      "Epoch: 142 Train Loss: 0.7172609748779954 Train Accuracy: 74.2375\n",
      "Epoch: 143 Train Loss: 0.7182824797356068 Train Accuracy: 72.1625\n",
      "Epoch: 144 Train Loss: 0.7091081828449601 Train Accuracy: 74.68125\n",
      "Epoch: 145 Train Loss: 0.7098646999617765 Train Accuracy: 72.71875\n",
      "Epoch: 146 Train Loss: 0.699739370380484 Train Accuracy: 75.21875\n",
      "Epoch: 147 Train Loss: 0.7002143677884322 Train Accuracy: 73.3125\n",
      "Epoch: 148 Train Loss: 0.6896217784995021 Train Accuracy: 75.81875\n",
      "Epoch: 149 Train Loss: 0.6898824627287742 Train Accuracy: 74.0375\n",
      "Epoch: 150 Train Loss: 0.6792698197784507 Train Accuracy: 76.23124999999999\n",
      "Epoch: 151 Train Loss: 0.6794175394718206 Train Accuracy: 74.825\n",
      "Epoch: 152 Train Loss: 0.669103830502382 Train Accuracy: 76.6875\n",
      "Epoch: 153 Train Loss: 0.6692340874198165 Train Accuracy: 75.5625\n",
      "Epoch: 154 Train Loss: 0.6593974143223232 Train Accuracy: 77.15625\n",
      "Epoch: 155 Train Loss: 0.6595801653077551 Train Accuracy: 76.225\n",
      "Epoch: 156 Train Loss: 0.6502891930998845 Train Accuracy: 77.4625\n",
      "Epoch: 157 Train Loss: 0.6505653284924894 Train Accuracy: 76.76875\n",
      "Epoch: 158 Train Loss: 0.6418208625523737 Train Accuracy: 77.925\n",
      "Epoch: 159 Train Loss: 0.6422065925391887 Train Accuracy: 77.2\n",
      "Epoch: 160 Train Loss: 0.6339758224240467 Train Accuracy: 78.2\n",
      "Epoch: 161 Train Loss: 0.6344689261840226 Train Accuracy: 77.56875\n",
      "Epoch: 162 Train Loss: 0.6267081550367007 Train Accuracy: 78.55\n",
      "Epoch: 163 Train Loss: 0.6272933203489925 Train Accuracy: 77.8125\n",
      "Epoch: 164 Train Loss: 0.6199610358795183 Train Accuracy: 78.8625\n",
      "Epoch: 165 Train Loss: 0.6206136545275804 Train Accuracy: 78.05\n",
      "Epoch: 166 Train Loss: 0.6136771297046566 Train Accuracy: 78.95\n",
      "Epoch: 167 Train Loss: 0.6143658503030276 Train Accuracy: 78.28125\n",
      "Epoch: 168 Train Loss: 0.6078039222846607 Train Accuracy: 79.25\n",
      "Epoch: 169 Train Loss: 0.6084924991453663 Train Accuracy: 78.55625\n",
      "Epoch: 170 Train Loss: 0.6022962884291263 Train Accuracy: 79.53750000000001\n",
      "Epoch: 171 Train Loss: 0.6029452028750506 Train Accuracy: 78.91875\n",
      "Epoch: 172 Train Loss: 0.5971178467906918 Train Accuracy: 79.65\n",
      "Epoch: 173 Train Loss: 0.5976860159018009 Train Accuracy: 79.0625\n",
      "Epoch: 174 Train Loss: 0.5922421019882813 Train Accuracy: 79.7875\n",
      "Epoch: 175 Train Loss: 0.5926887987951206 Train Accuracy: 79.21875\n",
      "Epoch: 176 Train Loss: 0.5876540566442227 Train Accuracy: 79.89375\n",
      "Epoch: 177 Train Loss: 0.5879409523202717 Train Accuracy: 79.425\n",
      "Epoch: 178 Train Loss: 0.5833528411616321 Train Accuracy: 80.05624999999999\n",
      "Epoch: 179 Train Loss: 0.5834458170283534 Train Accuracy: 79.55624999999999\n",
      "Epoch: 180 Train Loss: 0.5793558835820862 Train Accuracy: 80.075\n",
      "Epoch: 181 Train Loss: 0.5792258801676701 Train Accuracy: 79.7875\n",
      "Epoch: 182 Train Loss: 0.5757050981126041 Train Accuracy: 80.125\n",
      "Epoch: 183 Train Loss: 0.5753266263918363 Train Accuracy: 79.825\n",
      "Epoch: 184 Train Loss: 0.5724752098716467 Train Accuracy: 80.10625\n",
      "Epoch: 185 Train Loss: 0.5718200059856986 Train Accuracy: 79.83749999999999\n",
      "Epoch: 186 Train Loss: 0.5697829336107924 Train Accuracy: 79.98125\n",
      "Epoch: 187 Train Loss: 0.5688043943706618 Train Accuracy: 79.925\n",
      "Epoch: 188 Train Loss: 0.567791792397204 Train Accuracy: 79.9875\n",
      "Epoch: 189 Train Loss: 0.5663938819816436 Train Accuracy: 79.94375000000001\n",
      "Epoch: 190 Train Loss: 0.5666988195151952 Train Accuracy: 79.88125000000001\n",
      "Epoch: 191 Train Loss: 0.5646847193161894 Train Accuracy: 79.8875\n",
      "Epoch: 192 Train Loss: 0.5666768272302886 Train Accuracy: 79.675\n",
      "Epoch: 193 Train Loss: 0.5636876834295101 Train Accuracy: 79.9875\n",
      "Epoch: 194 Train Loss: 0.5677433656092542 Train Accuracy: 79.48125\n",
      "Epoch: 195 Train Loss: 0.5632375727876513 Train Accuracy: 79.95625\n",
      "Epoch: 196 Train Loss: 0.5695733869531617 Train Accuracy: 79.21875\n",
      "Epoch: 197 Train Loss: 0.5629399475640339 Train Accuracy: 79.81875000000001\n",
      "Epoch: 198 Train Loss: 0.5713893900449639 Train Accuracy: 79.06875\n",
      "Epoch: 199 Train Loss: 0.5622391974822936 Train Accuracy: 79.7625\n",
      "Epoch: 200 Train Loss: 0.5721315410931843 Train Accuracy: 79.05624999999999\n",
      "Epoch: 201 Train Loss: 0.5606137664931213 Train Accuracy: 79.83125\n",
      "Epoch: 202 Train Loss: 0.5709164424028101 Train Accuracy: 79.23125\n",
      "Epoch: 203 Train Loss: 0.5577820116424849 Train Accuracy: 79.95625\n",
      "Epoch: 204 Train Loss: 0.5674638439559735 Train Accuracy: 79.4\n",
      "Epoch: 205 Train Loss: 0.5537931740045354 Train Accuracy: 80.15625\n",
      "Epoch: 206 Train Loss: 0.5621645924726847 Train Accuracy: 79.7375\n",
      "Epoch: 207 Train Loss: 0.5489715947777559 Train Accuracy: 80.45\n",
      "Epoch: 208 Train Loss: 0.555798622675736 Train Accuracy: 80.05\n",
      "Epoch: 209 Train Loss: 0.5437640770996753 Train Accuracy: 80.75\n",
      "Epoch: 210 Train Loss: 0.549161663715248 Train Accuracy: 80.33749999999999\n",
      "Epoch: 211 Train Loss: 0.5385800042810909 Train Accuracy: 81.10000000000001\n",
      "Epoch: 212 Train Loss: 0.542828305501441 Train Accuracy: 80.63749999999999\n",
      "Epoch: 213 Train Loss: 0.5336997267999766 Train Accuracy: 81.2875\n",
      "Epoch: 214 Train Loss: 0.5371033023735651 Train Accuracy: 80.9375\n",
      "Epoch: 215 Train Loss: 0.5292648950056966 Train Accuracy: 81.45625\n",
      "Epoch: 216 Train Loss: 0.5320858070030151 Train Accuracy: 81.11874999999999\n",
      "Epoch: 217 Train Loss: 0.5253152167426005 Train Accuracy: 81.65\n",
      "Epoch: 218 Train Loss: 0.5277576138238618 Train Accuracy: 81.3125\n",
      "Epoch: 219 Train Loss: 0.5218327422604201 Train Accuracy: 81.875\n",
      "Epoch: 220 Train Loss: 0.5240501791319057 Train Accuracy: 81.49374999999999\n",
      "Epoch: 221 Train Loss: 0.5187747117328176 Train Accuracy: 82.04375\n",
      "Epoch: 222 Train Loss: 0.5208829681368953 Train Accuracy: 81.5375\n",
      "Epoch: 223 Train Loss: 0.5160923495514036 Train Accuracy: 82.175\n",
      "Epoch: 224 Train Loss: 0.5181807896658414 Train Accuracy: 81.63125\n",
      "Epoch: 225 Train Loss: 0.5137393491131255 Train Accuracy: 82.33749999999999\n",
      "Epoch: 226 Train Loss: 0.5158790716076918 Train Accuracy: 81.69375\n",
      "Epoch: 227 Train Loss: 0.5116742712524985 Train Accuracy: 82.3875\n",
      "Epoch: 228 Train Loss: 0.5139231775305994 Train Accuracy: 81.76875\n",
      "Epoch: 229 Train Loss: 0.509859809949702 Train Accuracy: 82.35625\n",
      "Epoch: 230 Train Loss: 0.5122651531922424 Train Accuracy: 81.825\n",
      "Epoch: 231 Train Loss: 0.5082606791482018 Train Accuracy: 82.44375000000001\n",
      "Epoch: 232 Train Loss: 0.510859655021468 Train Accuracy: 81.95\n",
      "Epoch: 233 Train Loss: 0.5068411541001806 Train Accuracy: 82.53125\n",
      "Epoch: 234 Train Loss: 0.5096600587258475 Train Accuracy: 81.95625\n",
      "Epoch: 235 Train Loss: 0.5055629720752921 Train Accuracy: 82.54375\n",
      "Epoch: 236 Train Loss: 0.5086155036407507 Train Accuracy: 81.925\n",
      "Epoch: 237 Train Loss: 0.5043841694540019 Train Accuracy: 82.59375\n",
      "Epoch: 238 Train Loss: 0.5076695751133253 Train Accuracy: 81.8875\n",
      "Epoch: 239 Train Loss: 0.5032593227676484 Train Accuracy: 82.61874999999999\n",
      "Epoch: 240 Train Loss: 0.5067612052249768 Train Accuracy: 81.89999999999999\n",
      "Epoch: 241 Train Loss: 0.5021414334686626 Train Accuracy: 82.61874999999999\n",
      "Epoch: 242 Train Loss: 0.5058280063836105 Train Accuracy: 81.8875\n",
      "Epoch: 243 Train Loss: 0.5009853015036141 Train Accuracy: 82.60625\n",
      "Epoch: 244 Train Loss: 0.5048116167223199 Train Accuracy: 81.83125\n",
      "Epoch: 245 Train Loss: 0.49975175054402005 Train Accuracy: 82.625\n",
      "Epoch: 246 Train Loss: 0.5036639119528826 Train Accuracy: 81.875\n",
      "Epoch: 247 Train Loss: 0.49841169185423284 Train Accuracy: 82.69999999999999\n",
      "Epoch: 248 Train Loss: 0.5023524654408427 Train Accuracy: 81.90625\n",
      "Epoch: 249 Train Loss: 0.4969489523552146 Train Accuracy: 82.7375\n",
      "Epoch: 250 Train Loss: 0.5008637232967357 Train Accuracy: 81.96875\n",
      "Epoch: 251 Train Loss: 0.4953611251536051 Train Accuracy: 82.78125\n",
      "Epoch: 252 Train Loss: 0.49920304729166953 Train Accuracy: 82.0125\n",
      "Epoch: 253 Train Loss: 0.4936583023869678 Train Accuracy: 82.9125\n",
      "Epoch: 254 Train Loss: 0.49739177229290876 Train Accuracy: 82.05624999999999\n",
      "Epoch: 255 Train Loss: 0.49186015493405566 Train Accuracy: 83.00625000000001\n",
      "Epoch: 256 Train Loss: 0.49546227027484746 Train Accuracy: 82.125\n",
      "Epoch: 257 Train Loss: 0.4899921863744985 Train Accuracy: 83.11874999999999\n",
      "Epoch: 258 Train Loss: 0.4934523765083601 Train Accuracy: 82.2625\n",
      "Epoch: 259 Train Loss: 0.48808201713320004 Train Accuracy: 83.18124999999999\n",
      "Epoch: 260 Train Loss: 0.49140037533326514 Train Accuracy: 82.35\n",
      "Epoch: 261 Train Loss: 0.48615632378551293 Train Accuracy: 83.2\n",
      "Epoch: 262 Train Loss: 0.48934126744712536 Train Accuracy: 82.4125\n",
      "Epoch: 263 Train Loss: 0.4842387262134579 Train Accuracy: 83.25625\n",
      "Epoch: 264 Train Loss: 0.4873045191562209 Train Accuracy: 82.5125\n",
      "Epoch: 265 Train Loss: 0.4823486231309442 Train Accuracy: 83.34375\n",
      "Epoch: 266 Train Loss: 0.4853131137269543 Train Accuracy: 82.5875\n",
      "Epoch: 267 Train Loss: 0.4805007958943175 Train Accuracy: 83.45\n",
      "Epoch: 268 Train Loss: 0.4833835430099366 Train Accuracy: 82.69375\n",
      "Epoch: 269 Train Loss: 0.47870553505175556 Train Accuracy: 83.5375\n",
      "Epoch: 270 Train Loss: 0.4815263577144864 Train Accuracy: 82.75625\n",
      "Epoch: 271 Train Loss: 0.4769690598819912 Train Accuracy: 83.55625\n",
      "Epoch: 272 Train Loss: 0.4797469672135416 Train Accuracy: 82.8125\n",
      "Epoch: 273 Train Loss: 0.4752940571539346 Train Accuracy: 83.6375\n",
      "Epoch: 274 Train Loss: 0.4780464829190066 Train Accuracy: 82.8875\n",
      "Epoch: 275 Train Loss: 0.473680230136328 Train Accuracy: 83.7125\n",
      "Epoch: 276 Train Loss: 0.47642249517728597 Train Accuracy: 82.9375\n",
      "Epoch: 277 Train Loss: 0.4721248049166644 Train Accuracy: 83.76249999999999\n",
      "Epoch: 278 Train Loss: 0.4748697461425182 Train Accuracy: 83.0625\n",
      "Epoch: 279 Train Loss: 0.4706229816681101 Train Accuracy: 83.79375\n",
      "Epoch: 280 Train Loss: 0.4733807076302272 Train Accuracy: 83.13125\n",
      "Epoch: 281 Train Loss: 0.46916834323057977 Train Accuracy: 83.825\n",
      "Epoch: 282 Train Loss: 0.4719460970458722 Train Accuracy: 83.175\n",
      "Epoch: 283 Train Loss: 0.46775324447341027 Train Accuracy: 83.825\n",
      "Epoch: 284 Train Loss: 0.4705553711068494 Train Accuracy: 83.19375\n",
      "Epoch: 285 Train Loss: 0.46636920622889627 Train Accuracy: 83.85000000000001\n",
      "Epoch: 286 Train Loss: 0.46919723081517484 Train Accuracy: 83.22500000000001\n",
      "Epoch: 287 Train Loss: 0.4650073299201821 Train Accuracy: 83.89375\n",
      "Epoch: 288 Train Loss: 0.46786015598231334 Train Accuracy: 83.28750000000001\n",
      "Epoch: 289 Train Loss: 0.4636587361488205 Train Accuracy: 83.94375\n",
      "Epoch: 290 Train Loss: 0.4665329674535719 Train Accuracy: 83.30625\n",
      "Epoch: 291 Train Loss: 0.46231501549515547 Train Accuracy: 84.03125\n",
      "Epoch: 292 Train Loss: 0.4652053942627502 Train Accuracy: 83.33125\n",
      "Epoch: 293 Train Loss: 0.46096866593487307 Train Accuracy: 84.11874999999999\n",
      "Epoch: 294 Train Loss: 0.4638686057765841 Train Accuracy: 83.38125\n",
      "Epoch: 295 Train Loss: 0.4596134818338398 Train Accuracy: 84.16875\n",
      "Epoch: 296 Train Loss: 0.4625156595689415 Train Accuracy: 83.4375\n",
      "Epoch: 297 Train Loss: 0.4582448569541061 Train Accuracy: 84.23125\n",
      "Epoch: 298 Train Loss: 0.4611418169478803 Train Accuracy: 83.5375\n",
      "Epoch: 299 Train Loss: 0.45685996931195466 Train Accuracy: 84.28125\n",
      "Epoch: 300 Train Loss: 0.459744689992178 Train Accuracy: 83.60625\n",
      "Epoch: 301 Train Loss: 0.455457828180231 Train Accuracy: 84.29375\n",
      "Epoch: 302 Train Loss: 0.45832420415672714 Train Accuracy: 83.65\n",
      "Epoch: 303 Train Loss: 0.4540391803371244 Train Accuracy: 84.375\n",
      "Epoch: 304 Train Loss: 0.4568823843511262 Train Accuracy: 83.72500000000001\n",
      "Epoch: 305 Train Loss: 0.45260629009069714 Train Accuracy: 84.38125\n",
      "Epoch: 306 Train Loss: 0.45542299446640294 Train Accuracy: 83.83125\n",
      "Epoch: 307 Train Loss: 0.45116262195066315 Train Accuracy: 84.40625\n",
      "Epoch: 308 Train Loss: 0.4539510760342179 Train Accuracy: 83.89375\n",
      "Epoch: 309 Train Loss: 0.4497124634551357 Train Accuracy: 84.4375\n",
      "Epoch: 310 Train Loss: 0.45247243853900637 Train Accuracy: 83.9375\n",
      "Epoch: 311 Train Loss: 0.44826052765585084 Train Accuracy: 84.46875\n",
      "Epoch: 312 Train Loss: 0.45099315186183664 Train Accuracy: 84.025\n",
      "Epoch: 313 Train Loss: 0.44681157087172285 Train Accuracy: 84.52499999999999\n",
      "Epoch: 314 Train Loss: 0.4495190824893058 Train Accuracy: 84.11874999999999\n",
      "Epoch: 315 Train Loss: 0.44537005345997727 Train Accuracy: 84.5875\n",
      "Epoch: 316 Train Loss: 0.4480555026392191 Train Accuracy: 84.15625\n",
      "Epoch: 317 Train Loss: 0.44393986186778417 Train Accuracy: 84.64375\n",
      "Epoch: 318 Train Loss: 0.4466067885144856 Train Accuracy: 84.19375\n",
      "Epoch: 319 Train Loss: 0.442524101183364 Train Accuracy: 84.7125\n",
      "Epoch: 320 Train Loss: 0.445176212879272 Train Accuracy: 84.3125\n",
      "Epoch: 321 Train Loss: 0.44112496016624114 Train Accuracy: 84.75\n",
      "Epoch: 322 Train Loss: 0.44376582928467884 Train Accuracy: 84.38125\n",
      "Epoch: 323 Train Loss: 0.4397436458358702 Train Accuracy: 84.83125\n",
      "Epoch: 324 Train Loss: 0.4423764406806297 Train Accuracy: 84.44375\n",
      "Epoch: 325 Train Loss: 0.4383803819654364 Train Accuracy: 84.875\n",
      "Epoch: 326 Train Loss: 0.44100764316881574 Train Accuracy: 84.52499999999999\n",
      "Epoch: 327 Train Loss: 0.4370344646418352 Train Accuracy: 84.925\n",
      "Epoch: 328 Train Loss: 0.4396579352153218 Train Accuracy: 84.5875\n",
      "Epoch: 329 Train Loss: 0.4357043676223766 Train Accuracy: 84.95625\n",
      "Epoch: 330 Train Loss: 0.43832488265695035 Train Accuracy: 84.6125\n",
      "Epoch: 331 Train Loss: 0.43438788982194837 Train Accuracy: 85.00625000000001\n",
      "Epoch: 332 Train Loss: 0.43700532944984577 Train Accuracy: 84.6875\n",
      "Epoch: 333 Train Loss: 0.4330823364177689 Train Accuracy: 85.06875000000001\n",
      "Epoch: 334 Train Loss: 0.4356956428771472 Train Accuracy: 84.7\n",
      "Epoch: 335 Train Loss: 0.43178472360761894 Train Accuracy: 85.10625\n",
      "Epoch: 336 Train Loss: 0.43439197987688793 Train Accuracy: 84.73125\n",
      "Epoch: 337 Train Loss: 0.43049199518905 Train Accuracy: 85.1625\n",
      "Epoch: 338 Train Loss: 0.43309055872716434 Train Accuracy: 84.775\n",
      "Epoch: 339 Train Loss: 0.42920123731086285 Train Accuracy: 85.19375\n",
      "Epoch: 340 Train Loss: 0.4317879182913123 Train Accuracy: 84.83125\n",
      "Epoch: 341 Train Loss: 0.42790987660855817 Train Accuracy: 85.28125\n",
      "Epoch: 342 Train Loss: 0.4304811462382525 Train Accuracy: 84.8625\n",
      "Epoch: 343 Train Loss: 0.42661584707843825 Train Accuracy: 85.3125\n",
      "Epoch: 344 Train Loss: 0.42916805882098424 Train Accuracy: 84.91875\n",
      "Epoch: 345 Train Loss: 0.4253177128747589 Train Accuracy: 85.3375\n",
      "Epoch: 346 Train Loss: 0.4278473182518081 Train Accuracy: 84.91875\n",
      "Epoch: 347 Train Loss: 0.42401473778938725 Train Accuracy: 85.375\n",
      "Epoch: 348 Train Loss: 0.42651847927143327 Train Accuracy: 84.975\n",
      "Epoch: 349 Train Loss: 0.4227068971503958 Train Accuracy: 85.40625\n",
      "Epoch: 350 Train Loss: 0.4251819634685743 Train Accuracy: 85.0125\n",
      "Epoch: 351 Train Loss: 0.42139483356590207 Train Accuracy: 85.43125\n",
      "Epoch: 352 Train Loss: 0.42383896719996805 Train Accuracy: 85.05625\n",
      "Epoch: 353 Train Loss: 0.4200797634662283 Train Accuracy: 85.5125\n",
      "Epoch: 354 Train Loss: 0.4224913154186153 Train Accuracy: 85.16875\n",
      "Epoch: 355 Train Loss: 0.4187633459110056 Train Accuracy: 85.575\n",
      "Epoch: 356 Train Loss: 0.4211412783579314 Train Accuracy: 85.19375\n",
      "Epoch: 357 Train Loss: 0.4174475280037077 Train Accuracy: 85.60624999999999\n",
      "Epoch: 358 Train Loss: 0.4197913702680244 Train Accuracy: 85.25625\n",
      "Epoch: 359 Train Loss: 0.41613438222038424 Train Accuracy: 85.6625\n",
      "Epoch: 360 Train Loss: 0.4184441491846254 Train Accuracy: 85.29375\n",
      "Epoch: 361 Train Loss: 0.4148259501082486 Train Accuracy: 85.675\n",
      "Epoch: 362 Train Loss: 0.4171020344058189 Train Accuracy: 85.33125\n",
      "Epoch: 363 Train Loss: 0.41352410453115673 Train Accuracy: 85.70625\n",
      "Epoch: 364 Train Loss: 0.41576715461889735 Train Accuracy: 85.3375\n",
      "Epoch: 365 Train Loss: 0.41223043947573534 Train Accuracy: 85.78750000000001\n",
      "Epoch: 366 Train Loss: 0.4144412352098208 Train Accuracy: 85.35000000000001\n",
      "Epoch: 367 Train Loss: 0.41094619293707324 Train Accuracy: 85.82499999999999\n",
      "Epoch: 368 Train Loss: 0.41312552886547277 Train Accuracy: 85.40625\n",
      "Epoch: 369 Train Loss: 0.40967220502948043 Train Accuracy: 85.85625\n",
      "Epoch: 370 Train Loss: 0.41182078961721924 Train Accuracy: 85.41875\n",
      "Epoch: 371 Train Loss: 0.40840891051399625 Train Accuracy: 85.8875\n",
      "Epoch: 372 Train Loss: 0.41052728722727627 Train Accuracy: 85.46875\n",
      "Epoch: 373 Train Loss: 0.40715636254344756 Train Accuracy: 85.88125\n",
      "Epoch: 374 Train Loss: 0.40924485635768904 Train Accuracy: 85.5375\n",
      "Epoch: 375 Train Loss: 0.40591428262453005 Train Accuracy: 85.91250000000001\n",
      "Epoch: 376 Train Loss: 0.40797297324118453 Train Accuracy: 85.575\n",
      "Epoch: 377 Train Loss: 0.404682130547867 Train Accuracy: 85.95\n",
      "Epoch: 378 Train Loss: 0.40671085150692765 Train Accuracy: 85.6125\n",
      "Epoch: 379 Train Loss: 0.4034591872895493 Train Accuracy: 85.99375\n",
      "Epoch: 380 Train Loss: 0.4054575483258726 Train Accuracy: 85.63125000000001\n",
      "Epoch: 381 Train Loss: 0.4022446436045891 Train Accuracy: 86.05625\n",
      "Epoch: 382 Train Loss: 0.40421207208763654 Train Accuracy: 85.7125\n",
      "Epoch: 383 Train Loss: 0.4010376871996924 Train Accuracy: 86.10624999999999\n",
      "Epoch: 384 Train Loss: 0.40297348339015465 Train Accuracy: 85.74374999999999\n",
      "Epoch: 385 Train Loss: 0.3998375819857779 Train Accuracy: 86.14375\n",
      "Epoch: 386 Train Loss: 0.40174098220478166 Train Accuracy: 85.775\n",
      "Epoch: 387 Train Loss: 0.3986437339529809 Train Accuracy: 86.16875\n",
      "Epoch: 388 Train Loss: 0.4005139756342937 Train Accuracy: 85.82499999999999\n",
      "Epoch: 389 Train Loss: 0.39745573962990827 Train Accuracy: 86.2125\n",
      "Epoch: 390 Train Loss: 0.39929212261671204 Train Accuracy: 85.91875\n",
      "Epoch: 391 Train Loss: 0.39627341478245703 Train Accuracy: 86.24374999999999\n",
      "Epoch: 392 Train Loss: 0.3980753540891177 Train Accuracy: 85.95625\n",
      "Epoch: 393 Train Loss: 0.3950968028256194 Train Accuracy: 86.25625\n",
      "Epoch: 394 Train Loss: 0.3968638693081879 Train Accuracy: 85.9875\n",
      "Epoch: 395 Train Loss: 0.39392616418421594 Train Accuracy: 86.30624999999999\n",
      "Epoch: 396 Train Loss: 0.39565811100465914 Train Accuracy: 86.05000000000001\n",
      "Epoch: 397 Train Loss: 0.39276194936404224 Train Accuracy: 86.35625\n",
      "Epoch: 398 Train Loss: 0.3944587236242346 Train Accuracy: 86.08749999999999\n",
      "Epoch: 399 Train Loss: 0.3916047596326853 Train Accuracy: 86.4\n",
      "Epoch: 400 Train Loss: 0.39326649993180907 Train Accuracy: 86.1\n",
      "Epoch: 401 Train Loss: 0.39045529986493316 Train Accuracy: 86.45\n",
      "Epoch: 402 Train Loss: 0.39208232166461027 Train Accuracy: 86.1625\n"
     ]
    }
   ],
   "source": [
    "clf.fit(trainData,trainLabel,lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e50167b-e66f-488e-88bc-6e33f321b254",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.predict(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9954cb91-cc32-4f70-bed7-d93f308f8477",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Acc(y,t,size):\n",
    "    maxData = np.argmax(y,axis = 1)\n",
    "    maxLabel = np.argmax(t,axis = 1)\n",
    "    compare = np.equal(maxData,maxLabel)\n",
    "\n",
    "    count = np.sum(compare)\n",
    "    return (count/size)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e627d48e-3ddd-4385-af82-ab9afe8062b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Acc(testLabel,clf.predict(testData),testLabel.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c257cb99-8101-4a92-8e2b-4bf528b49e6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968e98e9-f7c7-47bd-9ffe-df61e7dd4aaa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
